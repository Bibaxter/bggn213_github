---
title: "class 7: Machine learning 1"
author: "Bryn Baxter (PID: A69038039)"
format: pdf
---

Today we will delve into unsupervised machine learning with an intial focus on clustering and dimensionality reduction.

Let's start by making up some data to cluster: 

The `rnorm()` function can help us here. 

```{r}
hist( rnorm(3000, mean=3))
```

Lets get some data centered at 3, -3  -3, 3. 

```{r}
rnorm(30, mean=3)
rnorm(30, mean=-3)
```

Lets put them together into a vector

```{r}
#Combine 30+3 values with the 30-3 values
x <- c(rnorm(30, mean=3), rnorm(30, mean=-3))

#Bind these values together 
z <- cbind(x=x, y=rev(x))
head(z)
```
now lets try plotting it
```{r}
plot(z)
```
## K-means

Now we can see how K-means clusters this data. The main function of K-means clustering in "base R" is called `kmeans()`. 

```{r}
km <- kmeans(z, centers = 2)
km
```

```{r}
attributes(km
           )
```

>Q. what size is each cluster?

```{r}
km$size
```

>Q. the cluster membership vector (i.e. the answer: cluster to wich each point is allocated)

```{r}
km$cluster
```

>Q. Cluster centers 

```{r}
km$centers
```

>Q. Make a results figure, i.e. plot the data `z` colored by cluster membership and show the cluster centers. 

```{r}
plot(z, col="blue")
```

```{r}
plot(z, col=c("red", "blue"))
```

You can specify color based on a number, where 1 is black and 2 is red

```{r}
plot(z, col=km$cluster )
points(km$centers, col="blue", pch=16)
```


>Q. Re-run your k-means clustering and as for 4 clusters and plot the results as above. 

```{r}
km4 <- kmeans(z, centers = 4)
km4
```

```{r}
plot(z, col=km4$cluster)
points(km4$centers, col="blue", pch=15)
```

##Hierarchical clustering

the main "base R" function for this is `hclust()`. Unlike `kmeans()` you cant just give your dataset as an input, you need to privide a distance matrix. 

We can use the `dist()` function for this

```{r}
d <- dist(z)
#hclust()
d
```


```{r}
hc <- hclust(d)
hc
```
There is a custom plot for hclust objects, lets see it. 

```{r}
plot(hc)
abline(h=8, col="red")
```

The function to extract clusters/grps from hclust object/tree is called `cutree()`

```{r}
grps <- cutree(hc, h=8)
grps
```

>Q plot data with hclust clusters:

```{r}
plot(z, col=grps)
```

```{r}
cutree(hc, k=2)
```


## Principal Component Analysis (PCA)

Method to reduce the number of things you need to look at to something that looks more natural. 

In other words, reduce the features dimensionality while only losing a small amount of information. 

The first principal component (PC) follows a "best fit" through the data points.

The main function for PCA in base R for PAC is called `prcomp()`. There are many, many add on packages with PCA functions tailored to particular data types (RNAseq, protein, structure, metagenomics, etc....)

## PCA of UK food data 

Read the data into R, it is a CSV file and we can use `read.csv()` to read it 

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

I would like the food names as row names no their own column of data (first column currently). I can fix this like so:


```{r}
rownames(x) <- x[,1]
x
```

A better way to do this is to do it at the time of data import with `read.csv()`. 


```{r}
food <-  read.csv(url, row.names=1)
food
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(food)
```

Lets make some plots and dig into the data alittle. 
```{r}
rainbow(nrow(food))
```

```{r}
barplot(as.matrix(food), beside=T, col=rainbow(nrow(food)))
```
```{r}
t(food)
```

```{r}
barplot(as.matrix(t(food)), beside=T)
```
This is not helpful: 
```{r}
barplot(as.matrix(food), beside=F, col=rainbow(nrow(food)))

```
How about something called a "pairs" plot where we plot each country against all other countries. 

This pairwise analysis only works really if you have small data sets(ex: this data set only has 4 countries in it, if there was 1000 it would be very difficult to read)
```{r}
pairs(food, col=rainbow(nrow(food)), pch=16)
```

Really there is a better way...

## PCA to the rescue! 

We can run a Principal Component Analysis (PCA) for this data with the `prcomp()` function. 

```{r}
head(food)
```

We need to take a transpose of this data to get the food in the column and the countries in the rows. 

```{r}
pca <- prcomp(t(food))
summary(pca)
```

What is in my `pca` result object?

```{r}
attributes(pca)
```
The scores along the new PCs 
```{r}
pca$x
```
To make my main result figure, often called a PC plot (or score plot, orientation plot, or PC1 vs. PC2 plot etc. )

```{r}
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab= "PC2",
     col=c("orange","red", "blue", "darkgreen"), pch=16)
```

```{r}
library(ggplot2)

data <-as.data.frame(pca$x)

ggplot(data)+aes(PC1, PC2)+geom_point(col=c("orange","red", "blue", "darkgreen"), pch=16)
```

To see the contributions of the original variables (foods) to these new PCs we can look at the `pca$rotation` component of our result objects. 

```{r}
loadings <- as.data.frame(pca$rotation)
loadings$name <- rownames(loadings)

ggplot(loadings)+
  aes(PC1, name)+
geom_col()
```
And PC2

```{r}
ggplot(loadings)+
  aes(PC2, name)+
geom_col()
```

